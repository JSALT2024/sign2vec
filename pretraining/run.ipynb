{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_cpu_threads_per_process` was set to `12` to improve out-of-box performance when training on CPUs\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "None\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarahan-sahin\u001b[0m (\u001b[33mboun-pilab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/Users/karahansahin/Documents/Research/sign2vec/pretraining/wandb/run-20240619_094634-lagvyvl8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mprime-plasma-20\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/boun-pilab/sign2vec\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/boun-pilab/sign2vec/runs/lagvyvl8\u001b[0m\n",
      "Sign2VecConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_attn_dim\": null,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.42.0.dev0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "Sign2VecConfig {\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"adapter_attn_dim\": null,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.42.0.dev0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "Loaded 112111 samples\n",
      "With max_frames: 500 and stride: 1\n",
      "Loaded 112111 samples\n",
      "With max_frames: 500 and stride: 1\n",
      "DataLoader created successfully!\n",
      "torch.Size([8, 138000])\n",
      "torch.Size([8, 6899])\n",
      "====================\n",
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/karahansahin/Documents/Research/sign2vec/pretraining/run_sign2vec_pretraining.py\", line 68, in <module>\n",
      "    main()\n",
      "  File \"/Users/karahansahin/Documents/Research/sign2vec/pretraining/run_sign2vec_pretraining.py\", line 57, in main\n",
      "    trainer = Trainer(args,\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"/Users/karahansahin/Documents/Research/sign2vec/pretraining/utils/train.py\", line 60, in __init__\n",
      "    self.model.module.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\":False})\n",
      "    ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 222, in __getattr__\n",
      "    return getattr(self._orig_mod, name)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1709, in __getattr__\n",
      "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "AttributeError: 'Wav2Vec2ForPreTraining' object has no attribute 'module'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mprime-plasma-20\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/boun-pilab/sign2vec/runs/lagvyvl8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/boun-pilab/sign2vec\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240619_094634-lagvyvl8/logs\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\n",
      "    args.func(args)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 1097, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/accelerate/commands/launch.py\", line 703, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/opt/anaconda3/bin/python', 'run_sign2vec_pretraining.py', '--model_name_or_path=patrickvonplaten/wav2vec2-base-v2', '--output_dir=./sign2vec', '--max_train_steps=2', '--num_warmup_steps=3', '--gradient_accumulation_steps=4', '--learning_rate=0.001', '--weight_decay=0.01', '--max_duration_in_seconds=20.0', '--min_duration_in_seconds=2.0', '--logging_steps=1', '--saving_steps=10000', '--per_device_train_batch_size=8', '--per_device_eval_batch_size=8', '--adam_beta1=0.9', '--adam_beta2=0.98', '--adam_epsilon=1e-06', '--gradient_checkpointing', '--mask_time_prob=0.65', '--mask_time_length=10', '--use_face', '--use_hands', '--use_pose', '--train_info_path=../sign2vec/config/info.json', '--train_data_path=../sign2vec/features', '--validation_info_path=../sign2vec/config/info.json', '--validation_data_path=../sign2vec/features', '--config_name=config.json']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch run_sign2vec_pretraining.py \\\n",
    "\t\t\t\t\t\t--model_name_or_path=\"patrickvonplaten/wav2vec2-base-v2\" \\\n",
    "\t\t\t\t\t\t--output_dir=\"./sign2vec\" \\\n",
    "\t\t\t\t\t\t--max_train_steps=\"2\" \\\n",
    "\t\t\t\t\t\t--num_warmup_steps=\"3\" \\\n",
    "\t\t\t\t\t\t--gradient_accumulation_steps=\"4\" \\\n",
    "\t\t\t\t\t\t--learning_rate=\"0.001\" \\\n",
    "\t\t\t\t\t\t--weight_decay=\"0.01\" \\\n",
    "\t\t\t\t\t\t--max_duration_in_seconds=\"20.0\" \\\n",
    "\t\t\t\t\t\t--min_duration_in_seconds=\"2.0\" \\\n",
    "\t\t\t\t\t\t--logging_steps=\"1\" \\\n",
    "\t\t\t\t\t\t--saving_steps=\"10000\" \\\n",
    "\t\t\t\t\t\t--per_device_train_batch_size=\"8\" \\\n",
    "\t\t\t\t\t\t--per_device_eval_batch_size=\"8\" \\\n",
    "\t\t\t\t\t\t--adam_beta1=\"0.9\" \\\n",
    "\t\t\t\t\t\t--adam_beta2=\"0.98\" \\\n",
    "\t\t\t\t\t\t--adam_epsilon=\"1e-06\" \\\n",
    "\t\t\t\t\t\t--gradient_checkpointing \\\n",
    "\t\t\t\t\t\t--mask_time_prob=\"0.65\" \\\n",
    "\t\t\t\t\t\t--mask_time_length=\"10\" \\\n",
    "\t\t\t\t\t\t--use_face \\\n",
    "\t\t\t\t\t\t--use_hands \\\n",
    "\t\t\t\t\t\t--use_pose \\\n",
    "\t\t\t\t\t\t--train_info_path=\"../sign2vec/config/info.json\" \\\n",
    "\t\t\t\t\t\t--train_data_path=\"../sign2vec/features\" \\\n",
    "\t\t\t\t\t\t--validation_info_path=\"../sign2vec/config/info.json\" \\\n",
    "\t\t\t\t\t\t--validation_data_path=\"../sign2vec/features\" \\\n",
    "\t\t\t\t\t\t--config_name=\"config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.modeling_sign2vec import (\n",
    "    Sign2VecNoLayerNormConvLayer,\n",
    "    Sign2VecGroupNormConvLayer,\n",
    "    Sign2VecLayerNormConvLayer\n",
    ")\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class Sign2VecFeatureEncoder(nn.Module):\n",
    "    \"\"\"Construct the features from raw audio waveform\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        if config.feat_extract_norm == \"group\":\n",
    "            conv_layers = [Sign2VecGroupNormConvLayer(config, layer_id=0)] + [\n",
    "                Sign2VecNoLayerNormConvLayer(config, layer_id=i + 1) for i in range(config.num_feat_extract_layers - 1)\n",
    "            ]\n",
    "        elif config.feat_extract_norm == \"layer\":\n",
    "            conv_layers = [\n",
    "                Sign2VecLayerNormConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']\"\n",
    "            )\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        self.gradient_checkpointing = False\n",
    "        self._requires_grad = True\n",
    "\n",
    "    def _freeze_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._requires_grad = False\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        hidden_states = input_values[None, :]\n",
    "\n",
    "        # make sure hidden_states require grad for gradient_checkpointing\n",
    "        if self._requires_grad and self.training:\n",
    "            hidden_states.requires_grad = True\n",
    "\n",
    "        for conv_layer in self.conv_layers:\n",
    "            print('Layer input:', hidden_states.shape)\n",
    "            print('--->')\n",
    "            print(conv_layer)\n",
    "            print('--->')\n",
    "\n",
    "            if self._requires_grad and self.gradient_checkpointing and self.training:\n",
    "                hidden_states = self._gradient_checkpointing_func(\n",
    "                    conv_layer.__call__,\n",
    "                    hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = conv_layer(hidden_states)\n",
    "\n",
    "            print('Layer output:', hidden_states.shape)\n",
    "            print('======')\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2NoLayerNormConvLayer,\n",
    "    Wav2Vec2GroupNormConvLayer,\n",
    "    Wav2Vec2LayerNormConvLayer\n",
    ")\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class Wav2Vec2FeatureEncoder(nn.Module):\n",
    "    \"\"\"Construct the features from raw audio waveform\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        if config.feat_extract_norm == \"group\":\n",
    "            conv_layers = [Wav2Vec2GroupNormConvLayer(config, layer_id=0)] + [\n",
    "                Wav2Vec2NoLayerNormConvLayer(config, layer_id=i + 1) for i in range(config.num_feat_extract_layers - 1)\n",
    "            ]\n",
    "        elif config.feat_extract_norm == \"layer\":\n",
    "            conv_layers = [\n",
    "                Wav2Vec2LayerNormConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)\n",
    "            ]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']\"\n",
    "            )\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        self.gradient_checkpointing = False\n",
    "        self._requires_grad = True\n",
    "\n",
    "    def _freeze_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._requires_grad = False\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        hidden_states = input_values[None, :]\n",
    "\n",
    "        # make sure hidden_states require grad for gradient_checkpointing\n",
    "        if self._requires_grad and self.training:\n",
    "            hidden_states.requires_grad = True\n",
    "\n",
    "        for conv_layer in self.conv_layers:\n",
    "            print(hidden_states.shape, '--->')\n",
    "            print(conv_layer)\n",
    "            if self._requires_grad and self.gradient_checkpointing and self.training:\n",
    "                hidden_states = self._gradient_checkpointing_func(\n",
    "                    conv_layer.__call__,\n",
    "                    hidden_states,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = conv_layer(hidden_states)\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForPreTraining: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForPreTraining from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForPreTraining from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForPreTraining were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 93680])\n",
      "Layer out -> torch.Size([1, 512, 18735])\n",
      "Layer out -> torch.Size([1, 512, 9367])\n",
      "Layer out -> torch.Size([1, 512, 4683])\n",
      "Layer out -> torch.Size([1, 512, 2341])\n",
      "Layer out -> torch.Size([1, 512, 1170])\n",
      "Layer out -> torch.Size([1, 512, 585])\n",
      "Layer out -> torch.Size([1, 512, 292])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2ForPreTraining\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices\n",
    "from datasets import load_dataset\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "model = Wav2Vec2ForPreTraining.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "input_values = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values  # Batch size 1\n",
    "\n",
    "# compute masked indices\n",
    "batch_size, raw_sequence_length = input_values.shape\n",
    "sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length).item()\n",
    "mask_time_indices = _compute_mask_indices(\n",
    "    shape=(batch_size, sequence_length), mask_prob=0.2, mask_length=2\n",
    ")\n",
    "sampled_negative_indices = _sample_negative_indices(\n",
    "    features_shape=(batch_size, sequence_length),\n",
    "    num_negatives=model.config.num_negatives,\n",
    "    mask_time_indices=mask_time_indices,\n",
    ")\n",
    "mask_time_indices = torch.tensor(data=mask_time_indices, device=input_values.device, dtype=torch.long)\n",
    "sampled_negative_indices = torch.tensor(\n",
    "    data=sampled_negative_indices, device=input_values.device, dtype=torch.long\n",
    ")\n",
    "# for contrastive loss training model should be put into train mode\n",
    "\n",
    "print(input_values.shape)\n",
    "model = model.train()\n",
    "loss = model(\n",
    "    input_values, mask_time_indices=mask_time_indices, sampled_negative_indices=sampled_negative_indices\n",
    ").loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from utils.config import Sign2VecConfig\n",
    "from utils.modeling_sign2vec import Sign2VecFeatureEncoder\n",
    "\n",
    "config_dict = json.load(open(\"config.json\", \"r\"))\n",
    "\n",
    "config = Sign2VecConfig(**config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.modeling_sign2vec import Sign2VecForPreTraining\n",
    "from utils.bobsl import BOBSLDataset\n",
    "\n",
    "model = Sign2VecForPreTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channel_size = {\n",
    "    'face_keypoints_2d': 70,\n",
    "    'hand_left_keypoints_2d': 21,\n",
    "    'hand_right_keypoints_2d': 21,\n",
    "    'pose_keypoints_2d': 25,\n",
    "}\n",
    "\n",
    "\n",
    "# 1. Set the correct target sampling rate\n",
    "sampling_rate = int(\n",
    "    channel_size['pose_keypoints_2d'] * 2 +\n",
    "    channel_size['face_keypoints_2d'] * 2 + \n",
    "    channel_size['hand_left_keypoints_2d'] * 2  +\n",
    "    channel_size['hand_right_keypoints_2d'] * 2 \n",
    ") * 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOBSLDataset\n",
      "Sampling rate: 6850\n",
      "Max frame diff: 10\n",
      "Max length: 137000\n",
      "FPS: 25\n",
      "Max Frames: 500\n",
      "Info path: ../sign2vec/config/info.json\n",
      "Data path: ../sign2vec/features\n",
      "Loaded 9150 training samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:34: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:35: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:34: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:35: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/var/folders/nh/nft3_7kx3z7ght5v_h01w3v00000gn/T/ipykernel_75923/1488665431.py:34: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  mask_time_prob = config.mask_time_prob if 0.65 is None else 0.65\n",
      "/var/folders/nh/nft3_7kx3z7ght5v_h01w3v00000gn/T/ipykernel_75923/1488665431.py:35: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  mask_time_length = config.mask_time_length if 10 is None else 10\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from utils.feature_extraction_wav2vec2 import Sign2VecFeatureExtractor\n",
    "from utils.collator import DataCollatorForWav2Vec2Pretraining\n",
    "\n",
    "\n",
    "feature_extractor = Sign2VecFeatureExtractor(\n",
    "    feature_size=config.input_dim,\n",
    "    sampling_rate=sampling_rate,\n",
    "    padding_value=0,\n",
    "    do_normalize=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "vectorized_datasets = {\n",
    "    'train': BOBSLDataset(\n",
    "        data_path=\"../sign2vec/features\" ,\n",
    "        info_path=\"../sign2vec/config/info.json\",\n",
    "        use_face=True,\n",
    "        use_hands=True,\n",
    "        use_pose=True,\n",
    "        stride=20,\n",
    "        max_length=int(20) * sampling_rate,\n",
    "        sampling_rate=sampling_rate,\n",
    "        feature_extractor=feature_extractor,\n",
    "    )\n",
    "}\n",
    "\n",
    "# Activate gradient checkpointing if needed\n",
    "if True:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "# 4. Define data collator, optimizer and scheduler\n",
    "\n",
    "mask_time_prob = config.mask_time_prob if 0.65 is None else 0.65\n",
    "mask_time_length = config.mask_time_length if 10 is None else 10\n",
    "\n",
    "data_collator = DataCollatorForWav2Vec2Pretraining(\n",
    "    model=model,\n",
    "    feature_extractor=feature_extractor,\n",
    "    pad_to_multiple_of=None,\n",
    "    mask_time_prob=mask_time_prob,\n",
    "    mask_time_length=mask_time_length,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    vectorized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = next(iter(vectorized_datasets[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 500)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v['input_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 274, 500])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch.input_values.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sign2vec-pretrain] input_values torch.Size([8, 274, 500])\n",
      "[sign2vec] input_values torch.Size([8, 274, 500])\n",
      "[Sign2VecFeatureEncoder]: Forward\n",
      "Input values ->  torch.Size([8, 274, 500])\n",
      "Layer out ->  torch.Size([8, 512, 498])\n",
      "Layer out ->  torch.Size([8, 512, 166])\n",
      "Layer out ->  torch.Size([8, 512, 55])\n",
      "[sign2vec] extract_features torch.Size([8, 55, 512])\n"
     ]
    }
   ],
   "source": [
    "out = model(\n",
    "    **batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'projected_states', 'projected_quantized_states', 'codevector_perplexity', 'contrastive_loss', 'diversity_loss'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 55, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.projected_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
