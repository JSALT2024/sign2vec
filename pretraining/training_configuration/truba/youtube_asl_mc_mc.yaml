model_args:
  # Main Parameters
  diversity_loss_weight: 1.0
  num_codevector_groups: 2
  num_codevectors_per_group: 64
  conv_dim:
  - 512
  - 512
  - 512
  conv_kernel:
  - 5
  - 3
  - 1
  conv_stride:
  - 2
  - 1
  - 1
  # Multi-cue parameters
  use_multi_cue: true
  use_multi_constrastive_logits: false
  contrastive_loss_reduction: mean
  diversity_loss_reduction: mean
  # Additional parameters
  activation_dropout: 0.1
  adapter_attn_dim: null
  adapter_kernel_size: 3
  adapter_stride: 2
  add_adapter: false
  apply_spec_augment: true
  attention_dropout: 0.1
  bos_token_id: 1
  classifier_proj_size: 256
  codevector_dim: 256
  contrastive_logits_temperature: 0.1
  conv_bias: true
  do_stable_layer_norm: true
  eos_token_id: 2
  feat_extract_activation: gelu
  feat_extract_norm: layer
  feat_proj_dropout: 0.0
  feat_quantizer_dropout: 0.0
  final_dropout: 0.1
  hidden_act: gelu
  hidden_dropout: 0.1
  hidden_size: 768
  initializer_range: 0.02
  input_dim: 208
  intermediate_size: 3072
  layer_norm_eps: 1.0e-05
  layerdrop: 0.1
  mask_feature_length: 10
  mask_feature_min_masks: 0
  mask_feature_prob: 0.0
  mask_time_length: 10
  mask_time_min_masks: 2
  mask_time_prob: 0.05
  num_adapter_layers: 3
  num_attention_heads: 8
  num_conv_pos_embedding_groups: 16
  num_conv_pos_embeddings: 128
  num_hidden_layers: 12
  num_negatives: 100
  output_hidden_size: null
  pad_token_id: 0
  proj_codevector_dim: 256
  tdnn_dilation:
  - 1
  - 2
  - 3
  - 1
  - 1
  tdnn_dim:
  - 512
  - 512
  - 512
  - 512
  - 1500
  tdnn_kernel:
  - 5
  - 3
  - 3
  - 1
  - 1
  use_weighted_layer_sum: false
  vocab_size: 256
  xvector_output_dim: 512
training_params:
  # Run tag
  tags:
  - multi-cue
  - single-constrast
  - 64-codevectors
  - 2-codevector-groups
  - 0.5-diversity-loss-weight
  - diversity-loss-decay
  - truba
  - batch-size-64
  # The following parameters are used to train the model
  adam_beta1: 0.9
  adam_beta2: 0.98
  adam_epsilon: 1.0e-06
  weight_decay: 0.01
  learning_rate: 0.001
  max_train_steps: 25000
  num_warmup_steps: 5000
  per_device_eval_batch_size: 64
  per_device_train_batch_size: 64
  gradient_accumulation_steps: 4
  cache_dir: null
  # Sign2vec specific parameters
  kp_norm: true
  zero_mean_unit_var_norm: true
  # The directories should change according to the user's setup
  fps: 25
  env: server
  dataset_name: yasl
  data_dir: /localscratch/msaraclar/data
  train_data_path: /localscratch/msaraclar/data/train_dataset.csv
  validation_data_path: /localscratch/msaraclar/data/val_dataset.csv
  gradient_checkpointing: true
  run_name: sign2vec-yasl-truba
  hub_model_id: sign2vec-yasl-truba
  logging_steps: 1
  lr_scheduler_type: linear
  mask_time_prob: 0.65
  mask_time_length: 10
  # Gumbel softmax parameters - [will check out in future experiments]
  max_gumbel_temperature: 2.0
  min_gumbel_temperature: 0.5
  gumbel_temperature_decay: 0.99995
  # Diversity loss decay parameters
  min_diversity_loss_weight: 0.1
  # Additional parameters for the training
  max_duration_in_seconds: 20.0
  min_duration_in_seconds: 2.0
  model_name_or_path: patrickvonplaten/wav2vec2-base-v2
  num_train_epochs: 1000
  output_dir: ./sign2vec
  overwrite_cache: false
  pad_to_multiple_of: null
  preprocessing_num_workers: null
  preprocessing_only: false
  push_to_hub: true
  saving_steps: 10000
  seed: 42
  stride: 1
  validation_split_percentage: 1